{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ac3frxZMDnBf"
   },
   "source": [
    "# MSA 2023\n",
    "https://github.com/xylarshayu\n",
    "\n",
    "---\n",
    "\n",
    "1. MELD - Dataset that's been used. https://github.com/declare-lab/MELD\n",
    "2. Previous attempt (along with data analysis on dataset): https://colab.research.google.com/drive/1MYaJZZshcTsmVipJCqeHbdDpmX59QfmG?usp=sharing\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COvAEn0r3MBc"
   },
   "source": [
    "## Some knobs and dials for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xS-on37p3Tds"
   },
   "outputs": [],
   "source": [
    "DOWNLOAD_DATASET_FROM_SOURCE = False # Downloading the entire dataset from original source instead of just zip file from my drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFDEo4yr3HnA"
   },
   "source": [
    "## Getting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yUsprFrVQu16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (4.64.1)\n",
      "Collecting speechbrain\n",
      "  Downloading speechbrain-0.5.14-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.0/519.0 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting optuna\n",
      "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.12.0)\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Collecting hyperpyyaml (from speechbrain)\n",
      "  Downloading HyperPyYAML-1.2.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from speechbrain) (1.2.0)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from speechbrain) (1.7.3)\n",
      "Collecting sentencepiece (from speechbrain)\n",
      "  Downloading sentencepiece-0.1.98-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch>=1.9 (from speechbrain)\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio (from speechbrain)\n",
      "  Downloading torchaudio-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.10.4-py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (2.0.11)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from alembic>=1.5.0->optuna) (5.12.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.1.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch>=1.9->speechbrain)\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch>=1.9->speechbrain)\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.9->speechbrain)\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch>=1.9->speechbrain)\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->speechbrain) (67.7.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.9->speechbrain) (0.40.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
      "Collecting ruamel.yaml>=0.17.8 (from hyperpyyaml->speechbrain)\n",
      "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Collecting ruamel.yaml.clib>=0.2.6 (from ruamel.yaml>=0.17.8->hyperpyyaml->speechbrain)\n",
      "  Downloading ruamel.yaml.clib-0.2.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (500 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m500.1/500.1 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.1)\n",
      "Installing collected packages: tokenizers, sentencepiece, ruamel.yaml.clib, regex, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, colorlog, cmaes, ruamel.yaml, nvidia-cudnn-cu11, Mako, huggingface-hub, transformers, torch, hyperpyyaml, gdown, alembic, torchaudio, optuna, speechbrain\n",
      "Successfully installed Mako-1.2.4 alembic-1.10.4 cmaes-0.9.1 colorlog-6.7.0 gdown-4.7.1 huggingface-hub-0.14.1 hyperpyyaml-1.2.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 optuna-3.1.1 regex-2022.10.31 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.7 sentencepiece-0.1.98 speechbrain-0.5.14 tokenizers-0.13.3 torch-1.13.1 torchaudio-0.13.1 transformers-4.28.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown transformers tqdm speechbrain optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tarfile\n",
    "import shutil\n",
    "import zipfile\n",
    "import gdown\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MFy2MYu9DlHB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=11vTa_5JYNQmwM9F4dPXHMFBFoFXsRYfC\n",
      "From (redirected): https://drive.google.com/uc?id=11vTa_5JYNQmwM9F4dPXHMFBFoFXsRYfC&confirm=t&uuid=349836c4-0cfe-4a2f-a07b-c09cbb24c36b\n",
      "To: /home/jupyter/MELD.Raw.zip\n",
      "100%|██████████| 10.9G/10.9G [00:43<00:00, 251MB/s]\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract_meld_dataset():\n",
    "  subprocess.run([\"wget\", \"https://web.eecs.umich.edu/~mihalcea/downloads/MELD.Raw.tar.gz\"])\n",
    "\n",
    "  subprocess.run([\"tar\", \"-xzvpf\", \"MELD.Raw.tar.gz\", \"-C\", \".\"])\n",
    "  subprocess.run([\"tar\", \"-xzvf\", \"MELD.Raw/dev.tar.gz\", \"-C\", \"MELD.Raw/\"])\n",
    "  subprocess.run([\"tar\", \"-xzvf\", \"MELD.Raw/test.tar.gz\", \"-C\", \"MELD.Raw/\"])\n",
    "  subprocess.run([\"tar\", \"-xzvf\", \"MELD.Raw/train.tar.gz\", \"-C\", \"MELD.Raw/\"])\n",
    "\n",
    "  subprocess.run([\"rm\", \"MELD.Raw.tar.gz\"])\n",
    "  subprocess.run([\"rm\", \"MELD.Raw/dev.tar.gz\"])\n",
    "  subprocess.run([\"rm\", \"MELD.Raw/test.tar.gz\"])\n",
    "  subprocess.run([\"rm\", \"MELD.Raw/train.tar.gz\"])\n",
    "\n",
    "def download_meld_zip():\n",
    "  url = \"https://drive.google.com/uc?id=11vTa_5JYNQmwM9F4dPXHMFBFoFXsRYfC\"\n",
    "  gdown.download(url, output=\"MELD.Raw.zip\", quiet=False)\n",
    "  meld_zip_file = \"MELD.Raw.zip\"\n",
    "  with zipfile.ZipFile(meld_zip_file, 'r') as zip_ref:\n",
    "      zip_ref.extractall()\n",
    "  shutil.move(\"content/MELD.Raw\", \".\")\n",
    "  os.remove(meld_zip_file)\n",
    "  shutil.rmtree(\"content\")\n",
    "\n",
    "# Check if the MELD dataset already exists in the current directory\n",
    "if not os.path.isdir(\"MELD.Raw\"):\n",
    "    download_and_extract_meld_dataset() if DOWNLOAD_DATASET_FROM_SOURCE else download_meld_zip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki0KiA0FhSi1"
   },
   "source": [
    "## Data Preperation\n",
    "\n",
    "1. Importing CSV files\n",
    "2. Converting audio to embeddings using SpeechBrain\n",
    "3. Converting Text to embeddings using BERT\n",
    "4. Saving these neatly to drive as checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.10.0+cu113\n",
      "Uninstalling torch-1.10.0+cu113:\n",
      "  Successfully uninstalled torch-1.10.0+cu113\n",
      "Found existing installation: torchvision 0.11.1+cu113\n",
      "Uninstalling torchvision-0.11.1+cu113:\n",
      "  Successfully uninstalled torchvision-0.11.1+cu113\n",
      "Found existing installation: torchaudio 0.10.0+cu113\n",
      "Uninstalling torchaudio-0.10.0+cu113:\n",
      "  Successfully uninstalled torchaudio-0.10.0+cu113\n",
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
      "Collecting torch==1.10.0+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (1821.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/torchvision/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchvision==0.11.1+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.11.1%2Bcu113-cp37-cp37m-linux_x86_64.whl (24.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==0.10.0+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.10.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m758.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.10.0+cu113) (4.5.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.11.1+cu113) (1.21.6)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.11.1+cu113) (9.5.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.10.0+cu113 torchaudio-0.10.0+cu113 torchvision-0.11.1+cu113\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install --no-cache-dir torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zn3iS8SGWMHF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import datetime\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import multiprocessing\n",
    "from transformers import BertTokenizer\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "SB_model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"pretrained_models/spkrec-ecapa-voxceleb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuiTIz5rutRp"
   },
   "source": [
    "### Importing csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xaINeRqDuOCi"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('MELD.Raw/train_sent_emo.csv')\n",
    "dev_df = pd.read_csv('MELD.Raw/dev_sent_emo.csv')\n",
    "test_df = pd.read_csv('MELD.Raw/test_sent_emo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A9Mbf7TbWmOC"
   },
   "outputs": [],
   "source": [
    "#@title Remove all rows where the duration is greater than 5 seconds\n",
    "train_df['StartTime'] = pd.to_datetime(train_df['StartTime'])\n",
    "train_df['EndTime'] = pd.to_datetime(train_df['EndTime'])\n",
    "test_df['StartTime'] = pd.to_datetime(test_df['StartTime'])\n",
    "test_df['EndTime'] = pd.to_datetime(test_df['EndTime'])\n",
    "dev_df['StartTime'] = pd.to_datetime(dev_df['StartTime'])\n",
    "dev_df['EndTime'] = pd.to_datetime(dev_df['EndTime'])\n",
    "\n",
    "# Calculate the duration of each utterance as the difference between the 'EndTime' and 'StartTime' values\n",
    "train_df['Duration'] = train_df['EndTime'] - train_df['StartTime']\n",
    "test_df['Duration'] = test_df['EndTime'] - test_df['StartTime']\n",
    "dev_df['Duration'] = dev_df['EndTime'] - dev_df['StartTime']\n",
    "# Convert the 'Duration' column to seconds\n",
    "train_df['Duration'] = train_df['Duration'].dt.total_seconds()\n",
    "test_df['Duration'] = test_df['Duration'].dt.total_seconds()\n",
    "dev_df['Duration'] = dev_df['Duration'].dt.total_seconds()\n",
    "# Remove all rows where the duration is greater than 5 seconds\n",
    "train_df = train_df.drop(train_df[train_df['Duration'] >= 5].index)\n",
    "test_df = test_df.drop(test_df[test_df['Duration'] >= 5].index)\n",
    "dev_df = dev_df.drop(dev_df[dev_df['Duration'] >= 5].index)\n",
    "\n",
    "dataframes = { 'train': {'df':train_df, 'folder':'train_splits'}, 'dev': {'df':dev_df, 'folder':'dev_splits_complete'}, 'test': {'df': test_df, 'folder': 'output_repeated_splits_test'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RwNgUqakW9Rq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m testing how to print colored text\n"
     ]
    }
   ],
   "source": [
    "# Adding Colored Printing\n",
    "red = '\\033[91m'\n",
    "green = '\\033[92m'\n",
    "yellow = '\\033[93m'\n",
    "blue = '\\033[94m'\n",
    "pink = '\\033[95m'\n",
    "teal = '\\033[96m'\n",
    "grey = '\\033[97m'\n",
    "print(teal, \"testing how to print colored text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xZPkHSBurOX"
   },
   "source": [
    "### Extracting features from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting moviepy\n",
      "  Downloading moviepy-1.0.3.tar.gz (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting decorator<5.0,>=4.0.2 (from moviepy)\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/lib/python3.7/site-packages (from moviepy) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /opt/conda/lib/python3.7/site-packages (from moviepy) (2.28.2)\n",
      "Collecting proglog<=1.0.0 (from moviepy)\n",
      "  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from moviepy) (1.21.6)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /opt/conda/lib/python3.7/site-packages (from moviepy) (2.28.0)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n",
      "  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.7/site-packages (from imageio<3.0,>=2.5->moviepy) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.12.7)\n",
      "Building wheels for collected packages: moviepy\n",
      "  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110730 sha256=8993e559f5229b55932c12cdaceef47f9a966cdf246712f65a118dca113ce92d\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/56/dc/2b/9cd600d483c04af3353d66623056fc03faed76b7518faae4df\n",
      "Successfully built moviepy\n",
      "Installing collected packages: proglog, imageio_ffmpeg, decorator, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "Successfully installed decorator-4.4.2 imageio_ffmpeg-0.4.8 moviepy-1.0.3 proglog-0.1.10\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wRe5f5qoXOct"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from moviepy.editor import AudioFileClip\n",
    "\n",
    "data_dict_train = {}\n",
    "data_dict_dev = {}\n",
    "data_dict_test = {}\n",
    "data_dicts = {'train': data_dict_train, 'dev': data_dict_dev, 'test': data_dict_test}\n",
    "\n",
    "sentiment_encoder = LabelEncoder()\n",
    "emotion_encoder = LabelEncoder()\n",
    "sentiment_encoder.fit(train_df['Sentiment'])\n",
    "emotion_encoder.fit(train_df['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "336osDUfLiYy"
   },
   "outputs": [],
   "source": [
    "# Processes single data sample\n",
    "def process_file(dataset, sr_no):\n",
    "    success = True\n",
    "    folder_name = dataframes[dataset]['folder']\n",
    "    df = dataframes[dataset]['df']\n",
    "    dialogue_id, utterance_id = df.loc[sr_no, 'Dialogue_ID'], df.loc[sr_no, 'Utterance_ID']\n",
    "    file_path = f\"MELD.Raw/{folder_name}/dia{dialogue_id}_utt{utterance_id}.mp4\"\n",
    "    raw_text = df.loc[sr_no, 'Utterance']\n",
    "\n",
    "    try:\n",
    "      raw_audio = AudioFileClip(file_path).to_soundarray(fps=22000)\n",
    "      if raw_audio.shape[1] > 1:\n",
    "            raw_audio = raw_audio.mean(axis=1)\n",
    "      raw_audio = torch.tensor(raw_audio).unsqueeze(0)\n",
    "    except Exception as e:\n",
    "      print(yellow, \"Error loading file \", file_path)\n",
    "      print(red, type(e), \":\\n\", e)\n",
    "      success = False\n",
    "      return success, None, None, None, None\n",
    "    \n",
    "    audio_features = SB_model.encode_batch(raw_audio).mean(dim=1).squeeze(0)\n",
    "    text_features = tokenizer(raw_text, padding='max_length', truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    sentiment_label = sentiment_encoder.transform([df.loc[sr_no, 'Sentiment']])[0]\n",
    "    emotion_label = emotion_encoder.transform([df.loc[sr_no, 'Emotion']])[0]\n",
    "    \n",
    "    return success, audio_features, text_features, sentiment_label, emotion_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ymj6M2PTLjVD"
   },
   "outputs": [],
   "source": [
    "# Helper function to run process_file in parallel\n",
    "import funcs\n",
    "\n",
    "def process_files_in_parallel(dataset):\n",
    "    df = dataframes[dataset]['df']\n",
    "    folder_name = dataframes[dataset]['folder']\n",
    "    successful_indices = []\n",
    "    \n",
    "    print(teal, \"Processing files in parallel\")\n",
    "    if __name__ == '__main__':\n",
    "        with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "            pbar = tqdm(total=df.shape[0])\n",
    "            for sr_no in df.index:\n",
    "                print(yellow, \"Processing file \", sr_no)\n",
    "                \n",
    "                success, raw_audio, text_features, sentiment_label, emotion_label = executor.submit(funcs.process_file, folder_name, df, sr_no, sentiment_encoder, emotion_encoder, tokenizer).result()\n",
    "                if success:\n",
    "                    audio_features = SB_model.encode_batch(raw_audio).mean(dim=1).squeeze(0)\n",
    "                    print(blue, \"Audio features encoded\")\n",
    "                    data_dicts[dataset][sr_no] = {'audio': audio_features, 'text': text_features, 'sentiment': sentiment_label, 'emotion': emotion_label}\n",
    "                    successful_indices.append(sr_no)\n",
    "                pbar.update()\n",
    "    \n",
    "    pbar.close()\n",
    "    dataframes[dataset]['df'] = df.loc[successful_indices]\n",
    "\n",
    "    # Save checkpoints\n",
    "    df.to_csv(f'updated_{dataset}_df.csv', index=False)\n",
    "    with open(f'data_dict_{dataset}.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_dicts[dataset], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_sequentially(dataset):\n",
    "    df = dataframes[dataset]['df']\n",
    "    successful_indices = []\n",
    "    print(teal, \"Processing files in parallel\")\n",
    "    for sr_no in tqdm(df.index, desc=f\"Processing {dataset}\"):\n",
    "        success, audio_features, text_features, sentiment_label, emotion_label = process_file(dataset, sr_no)\n",
    "        if success:\n",
    "            data_dicts[dataset][sr_no] = {'audio': audio_features, 'text': text_features, 'sentiment': sentiment_label, 'emotion': emotion_label}\n",
    "            successful_indices.append(sr_no)\n",
    "    \n",
    "    dataframes[dataset]['df'] = df.loc[successful_indices]\n",
    "\n",
    "    # Save checkpoints\n",
    "    df.to_csv(f'updated_{dataset}_df.csv', index=False)\n",
    "    with open(f'data_dict_{dataset}.pickle', 'wb') as handle:\n",
    "        pickle.dump(data_dicts[dataset], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5E1UbygXOK07"
   },
   "outputs": [],
   "source": [
    "process_files_sequentially('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eH9N_YAbQYjf"
   },
   "outputs": [],
   "source": [
    "process_files_sequentially('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whroz07IQZ2C"
   },
   "outputs": [],
   "source": [
    "process_files_sequentially('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "36-sHrF3QbMT"
   },
   "outputs": [],
   "source": [
    "#@title Restore checkpoints after processing\n",
    "def load_data(dataset):\n",
    "    df = pd.read_csv(f'updated_{dataset}_df.csv')\n",
    "    with open(f'data_dict_{dataset}.pickle', 'rb') as handle:\n",
    "        data_dict = pickle.load(handle)\n",
    "    return df, data_dict\n",
    "\n",
    "train_df, data_dict_train = load_data('train')\n",
    "dev_df, data_dict_dev = load_data('dev')\n",
    "test_df, data_dict_test = load_data('test')\n",
    "\n",
    "data_dicts = {'train': data_dict_train, 'dev': data_dict_dev, 'test': data_dict_test}\n",
    "dataframes = { 'train': {'df':train_df, 'folder':'train_splits'}, 'dev': {'df':dev_df, 'folder':'dev_splits_complete'}, 'test': {'df': test_df, 'folder': 'output_repeated_splits_test'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(data_dicts['train'][12]['text']['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192])\n"
     ]
    }
   ],
   "source": [
    "print(data_dicts['train'][12]['audio'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "OILk5mu4nz6n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_dim:  768 \n",
      "audio_dim:  192\n"
     ]
    }
   ],
   "source": [
    "text_dim = 768 # Because BERT\n",
    "# audio_dim = data_dicts['train'][1]['audio'].shape[-1]\n",
    "audio_dim = 192 # Because speechbrain ECAPA-TDNN\n",
    "\n",
    "print(\"text_dim: \", text_dim, \"\\naudio_dim: \", audio_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1tYKw3aTwHQ"
   },
   "source": [
    "## Contrastive Learning\n",
    "\n",
    "1. Modality Fusion: Use an attention mechanism to combine the audio and text features for each data instance.\n",
    "\n",
    "2. Contrastive Learning: Use a Contrastive Learning approach to train a model that can distinguish between instances with the same emotion (positive pairs) and instances with different emotions (negative pairs).\n",
    "\n",
    "3. Model Architecture.\n",
    "\n",
    "4. Loss Function and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "FqoLKXMVTvQp"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = self.linear(x)\n",
    "        scores = F.softmax(scores, dim=-2) \n",
    "        weighted_sum = scores * x\n",
    "        return torch.sum(weighted_sum, dim=-2)\n",
    "\n",
    "\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, audio_dim, text_dim=768):  # Update default text_dim to 768 for BERT embeddings\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.audio_attention = Attention(audio_dim)\n",
    "        self.text_attention = Attention(text_dim)\n",
    "        self.fc = nn.Linear(audio_dim+text_dim, 256)\n",
    "\n",
    "    def forward(self, audio, text):\n",
    "        audio_repr = self.audio_attention(audio)\n",
    "        text_repr = self.text_attention(text)\n",
    "        combined = torch.cat((audio_repr, text_repr), dim=1)\n",
    "        out = self.fc(combined)\n",
    "        return F.normalize(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "EoNwTrDlcDpX"
   },
   "outputs": [],
   "source": [
    "#@title Contrastive Loss Function\n",
    "class TripletMarginLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletMarginLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = F.pairwise_distance(anchor, positive)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative)\n",
    "        loss = F.relu(pos_dist - neg_dist + self.margin)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLearningDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "        self.labels = np.array([item['emotion'] for item in data_list])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor = self.data_list[idx]\n",
    "        positive_idx = np.random.choice(np.where(self.labels == anchor['emotion'])[0])\n",
    "        negative_idx = np.random.choice(np.where(self.labels != anchor['emotion'])[0])\n",
    "        positive = self.data_list[positive_idx]\n",
    "        negative = self.data_list[negative_idx]\n",
    "\n",
    "        return (anchor['audio'].unsqueeze(0), anchor['text']['input_ids'].squeeze(-1)), anchor['emotion'], \\\n",
    "               (positive['audio'].unsqueeze(0), positive['text']['input_ids'].squeeze(-1)), \\\n",
    "               (negative['audio'].unsqueeze(0), negative['text']['input_ids'].squeeze(-1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_lists = {dataset: list(data_dicts[dataset].values()) for dataset in ['train', 'dev', 'test']}\n",
    "\n",
    "data_loaders = {dataset: DataLoader(ContrastiveLearningDataset(data_lists[dataset]), batch_size=32, shuffle=True, num_workers=num_cores) for dataset in ['train', 'dev', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b2dab1a7bb4f79a4b2f5ab162db638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-03 18:28:33,468]\u001b[0m A new study created in memory with name: no-name-54eb54cb-1789-45a8-8ad8-b1e39202cb7e\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 35.993\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 18:30:40,367]\u001b[0m Trial 0 finished with value: 20.28972750902176 and parameters: {'lr': 0.00011440199476855864, 'margin': 0.6993460620691053}. Best is trial 0 with value: 20.28972750902176.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  20.28972750902176\n",
      "\u001b[94m Best margin  0.6993460620691053\n",
      "\u001b[94m Best lr  0.00011440199476855864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 46.232\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 18:32:47,506]\u001b[0m Trial 1 finished with value: 20.28972750902176 and parameters: {'lr': 0.00011198152075917178, 'margin': 0.8924595126198741}. Best is trial 0 with value: 20.28972750902176.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 24.667\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 18:34:54,782]\u001b[0m Trial 2 finished with value: 11.91694888472557 and parameters: {'lr': 0.0008614685172137129, 'margin': 0.5027157614858289}. Best is trial 2 with value: 11.91694888472557.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  11.91694888472557\n",
      "\u001b[94m Best margin  0.5027157614858289\n",
      "\u001b[94m Best lr  0.0008614685172137129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 10.440\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 18:37:02,149]\u001b[0m Trial 3 finished with value: 5.762997433543205 and parameters: {'lr': 0.0053127879769354035, 'margin': 0.2024527666515041}. Best is trial 3 with value: 5.762997433543205.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  5.762997433543205\n",
      "\u001b[94m Best margin  0.2024527666515041\n",
      "\u001b[94m Best lr  0.0053127879769354035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 20.188\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 18:39:09,375]\u001b[0m Trial 4 finished with value: 5.762997433543205 and parameters: {'lr': 0.015190782425737995, 'margin': 0.3970048223060436}. Best is trial 3 with value: 5.762997433543205.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 18.483\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 18:41:16,704]\u001b[0m Trial 5 finished with value: 5.762997433543205 and parameters: {'lr': 0.0009594795404319104, 'margin': 0.38282449019923936}. Best is trial 3 with value: 5.762997433543205.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 9.053\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 18:43:24,000]\u001b[0m Trial 6 finished with value: 5.051897346973419 and parameters: {'lr': 0.0037478886079350036, 'margin': 0.17329740882958927}. Best is trial 6 with value: 5.051897346973419.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  5.051897346973419\n",
      "\u001b[94m Best margin  0.17329740882958927\n",
      "\u001b[94m Best lr  0.0037478886079350036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 24.495\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 18:45:31,230]\u001b[0m Trial 7 finished with value: 5.051897346973419 and parameters: {'lr': 0.0002599766645564623, 'margin': 0.47557957381487304}. Best is trial 6 with value: 5.051897346973419.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 9.190\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 18:47:38,394]\u001b[0m Trial 8 finished with value: 4.908187165856361 and parameters: {'lr': 0.003993793542077937, 'margin': 0.17762825676347993}. Best is trial 8 with value: 4.908187165856361.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  4.908187165856361\n",
      "\u001b[94m Best margin  0.17762825676347993\n",
      "\u001b[94m Best lr  0.003993793542077937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 39.642\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 18:49:45,702]\u001b[0m Trial 9 finished with value: 4.908187165856361 and parameters: {'lr': 0.09156417899892902, 'margin': 0.7865473298532892}. Best is trial 8 with value: 4.908187165856361.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 6.774\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 18:51:52,937]\u001b[0m Trial 10 finished with value: 3.775557555258274 and parameters: {'lr': 1.154327802455961e-05, 'margin': 0.11266765142051002}. Best is trial 10 with value: 3.775557555258274.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  3.775557555258274\n",
      "\u001b[94m Best margin  0.11266765142051002\n",
      "\u001b[94m Best lr  1.154327802455961e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 7.074\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 18:54:00,119]\u001b[0m Trial 11 finished with value: 3.775557555258274 and parameters: {'lr': 1.7154765652557325e-05, 'margin': 0.12132764661844389}. Best is trial 10 with value: 3.775557555258274.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 6.914\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 18:56:07,502]\u001b[0m Trial 12 finished with value: 3.5887641832232475 and parameters: {'lr': 1.1847026865847209e-05, 'margin': 0.11347280642662437}. Best is trial 12 with value: 3.5887641832232475.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  3.5887641832232475\n",
      "\u001b[94m Best margin  0.11347280642662437\n",
      "\u001b[94m Best lr  1.1847026865847209e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 15.400\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 18:58:14,779]\u001b[0m Trial 13 finished with value: 3.5887641832232475 and parameters: {'lr': 1.1098063302285339e-05, 'margin': 0.2968363240021017}. Best is trial 12 with value: 3.5887641832232475.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 15.160\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 19:00:22,010]\u001b[0m Trial 14 finished with value: 3.5887641832232475 and parameters: {'lr': 3.579568441283513e-05, 'margin': 0.29127001610670517}. Best is trial 12 with value: 3.5887641832232475.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 15.110\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 19:02:29,304]\u001b[0m Trial 15 finished with value: 3.5887641832232475 and parameters: {'lr': 1.0151116305639314e-05, 'margin': 0.2899052806722142}. Best is trial 12 with value: 3.5887641832232475.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 32.136\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 19:04:36,660]\u001b[0m Trial 16 finished with value: 3.5887641832232475 and parameters: {'lr': 3.945109975331213e-05, 'margin': 0.6162817141035974}. Best is trial 12 with value: 3.5887641832232475.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 13.982\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 19:06:43,897]\u001b[0m Trial 17 finished with value: 3.5887641832232475 and parameters: {'lr': 3.7291218216824424e-05, 'margin': 0.2686305273364953}. Best is trial 12 with value: 3.5887641832232475.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 20.387\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 19:08:51,171]\u001b[0m Trial 18 finished with value: 3.5887641832232475 and parameters: {'lr': 0.0002302090542239286, 'margin': 0.3952931292925728}. Best is trial 12 with value: 3.5887641832232475.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 6.494\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 19:10:58,500]\u001b[0m Trial 19 finished with value: 3.5798910707235336 and parameters: {'lr': 2.566965977751964e-05, 'margin': 0.10707275585447175}. Best is trial 19 with value: 3.5798910707235336.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  3.5798910707235336\n",
      "\u001b[94m Best margin  0.10707275585447175\n",
      "\u001b[94m Best lr  2.566965977751964e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 6.003\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 19:13:05,684]\u001b[0m Trial 20 finished with value: 3.5241395384073257 and parameters: {'lr': 6.455945961578103e-05, 'margin': 0.10010130752358352}. Best is trial 20 with value: 3.5241395384073257.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  3.5241395384073257\n",
      "\u001b[94m Best margin  0.10010130752358352\n",
      "\u001b[94m Best lr  6.455945961578103e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 6.477\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 19:15:12,999]\u001b[0m Trial 21 finished with value: 3.5241395384073257 and parameters: {'lr': 2.8398971682779253e-05, 'margin': 0.10700391727839839}. Best is trial 20 with value: 3.5241395384073257.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 11.493\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 19:17:20,331]\u001b[0m Trial 22 finished with value: 3.5241395384073257 and parameters: {'lr': 6.090223293766214e-05, 'margin': 0.2170202645297456}. Best is trial 20 with value: 3.5241395384073257.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 10.927\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 19:19:27,582]\u001b[0m Trial 23 finished with value: 3.5241395384073257 and parameters: {'lr': 7.07412028616547e-05, 'margin': 0.20924253690877315}. Best is trial 20 with value: 3.5241395384073257.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 11.823\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 19:21:34,810]\u001b[0m Trial 24 finished with value: 3.5241395384073257 and parameters: {'lr': 5.77365508093745e-05, 'margin': 0.2258777098123385}. Best is trial 20 with value: 3.5241395384073257.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 9.937\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 19:23:42,125]\u001b[0m Trial 25 finished with value: 3.5241395384073257 and parameters: {'lr': 2.5308272924300644e-05, 'margin': 0.18484532583689828}. Best is trial 20 with value: 3.5241395384073257.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 17.611\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "\u001b[32m[I 2023-05-03 19:25:49,488]\u001b[0m Trial 26 finished with value: 3.5241395384073257 and parameters: {'lr': 0.00019576744945731643, 'margin': 0.3454985586646844}. Best is trial 20 with value: 3.5241395384073257.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 12.709\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 19:27:56,745]\u001b[0m Trial 27 finished with value: 3.5241395384073257 and parameters: {'lr': 6.288192374575232e-05, 'margin': 0.24286919892319625}. Best is trial 20 with value: 3.5241395384073257.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 8.850\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 19:30:04,082]\u001b[0m Trial 28 finished with value: 3.5241395384073257 and parameters: {'lr': 0.0004955555815563593, 'margin': 0.1689009734814351}. Best is trial 20 with value: 3.5241395384073257.\u001b[0m\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 6.112\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "\u001b[32m[I 2023-05-03 19:32:11,406]\u001b[0m Trial 29 finished with value: 3.3454510271549225 and parameters: {'lr': 0.00012244720983334244, 'margin': 0.10518294052119545}. Best is trial 29 with value: 3.3454510271549225.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  3.3454510271549225\n",
      "\u001b[94m Best margin  0.10518294052119545\n",
      "\u001b[94m Best lr  0.00012244720983334244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 5.889\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  3.13708333671093\n",
      "\u001b[94m Best margin  0.10518294052119545\n",
      "\u001b[94m Best lr  0.00012244720983334244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  3.0420736894011497\n",
      "\u001b[94m Best margin  0.10518294052119545\n",
      "\u001b[94m Best lr  0.00012244720983334244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  2.8562482818961143\n",
      "\u001b[94m Best margin  0.10518294052119545\n",
      "\u001b[94m Best lr  0.00012244720983334244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] loss: 4.962\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  2.8064755760133266\n",
      "\u001b[94m Best margin  0.10518294052119545\n",
      "\u001b[94m Best lr  0.00012244720983334244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  2.592668764293194\n",
      "\u001b[94m Best margin  0.10518294052119545\n",
      "\u001b[94m Best lr  0.00012244720983334244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] loss: 4.799\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] loss: 4.684\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m New best validation loss achieved!  2.4147740341722965\n",
      "\u001b[94m Best margin  0.10518294052119545\n",
      "\u001b[94m Best lr  0.00012244720983334244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21] loss: 4.640\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26] loss: 4.454\n",
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n",
      "100%|██████████| 261/261 [01:53<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m Now running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4147740341722965"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "print_every = 5\n",
    "\n",
    "best_results = {'loss': float('inf'), 'model': None}\n",
    "\n",
    "def objective(trial, resume=False, num_epochs=1):\n",
    "    \n",
    "    contrastive_model = MultimodalModel(audio_dim=audio_dim, text_dim=text_dim)\n",
    "    if resume:\n",
    "        contrastive_model.load_state_dict(torch.load('contrastive_model.pt'))\n",
    "    contrastive_model.to(device)\n",
    "\n",
    "    # Define optimizer\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1) if not resume else study.best_params['lr']\n",
    "    optimizer = optim.Adam(contrastive_model.parameters(), lr=lr)\n",
    "\n",
    "    # Define loss function\n",
    "    margin = trial.suggest_uniform('margin', 0.1, 1.0) if not resume else study.best_params['margin']\n",
    "    criterion = TripletMarginLoss(margin=margin)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        contrastive_model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in tqdm(enumerate(data_loaders['train']), total=len(data_loaders['train'])):\n",
    "            (anchor_audio, anchor_text), anchor_label, (positive_audio, positive_text), (negative_audio, negative_text) = data\n",
    "\n",
    "            # Move tensors to the right device\n",
    "            anchor_audio = anchor_audio.to(device)\n",
    "            anchor_text = anchor_text.to(device)\n",
    "            positive_audio = positive_audio.to(device)\n",
    "            positive_text = positive_text.to(device)\n",
    "            negative_audio = negative_audio.to(device)\n",
    "            negative_text = negative_text.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                anchor_text = bert_model(anchor_text.squeeze(1))[0]\n",
    "                positive_text = bert_model(positive_text.squeeze(1))[0]\n",
    "                negative_text = bert_model(negative_text.squeeze(1))[0]\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward + backward + optimize\n",
    "            anchor_out = contrastive_model(anchor_audio, anchor_text)\n",
    "            positive_out = contrastive_model(positive_audio, positive_text)\n",
    "            negative_out = contrastive_model(negative_audio, negative_text)\n",
    "            loss = criterion(anchor_out, positive_out, negative_out)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        if epoch % print_every == 0:\n",
    "            print('[%d] loss: %.3f' % (epoch + 1, running_loss / print_every))\n",
    "            running_loss = 0.0\n",
    "\n",
    "        # Validation\n",
    "        print(teal, \"Now running validation...\")\n",
    "        contrastive_model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for i, data in tqdm(enumerate(data_loaders['dev']), total=len(data_loaders['dev'])):\n",
    "                (anchor_audio, anchor_text), anchor_label, (positive_audio, positive_text), (negative_audio, negative_text) = data\n",
    "\n",
    "                # Move tensors to the right device\n",
    "                anchor_audio = anchor_audio.to(device)\n",
    "                anchor_text = anchor_text.to(device)\n",
    "                positive_audio = positive_audio.to(device)\n",
    "                positive_text = positive_text.to(device)\n",
    "                negative_audio = negative_audio.to(device)\n",
    "                negative_text = negative_text.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    anchor_text = bert_model(anchor_text.squeeze(1))[0]\n",
    "                    positive_text = bert_model(positive_text.squeeze(1))[0]\n",
    "                    negative_text = bert_model(negative_text.squeeze(1))[0]\n",
    "\n",
    "                anchor_out = contrastive_model(anchor_audio, anchor_text)\n",
    "                positive_out = contrastive_model(positive_audio, positive_text)\n",
    "                negative_out = contrastive_model(negative_audio, negative_text)\n",
    "                loss = criterion(anchor_out, positive_out, negative_out)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        if val_loss < best_results['loss']:            \n",
    "            print(blue, \"New best validation loss achieved! \", val_loss) \n",
    "            print(blue, \"Best margin \", margin)\n",
    "            print(blue, \"Best lr \", lr)\n",
    "            best_results['loss'] = val_loss\n",
    "            best_results['model'] = contrastive_model.state_dict()\n",
    "            torch.save(best_results['model'], 'contrastive_model.pt')\n",
    "        if not resume and trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "            \n",
    "    return best_results['loss']\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "best_pretrained_params = study.best_params\n",
    "with open('best_pretrained_params.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_pretrained_params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "objective(None, resume=True, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "oEzUL2h2tCRM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [00:29<00:00,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#@title Evaluate Contrastive Model on test set\n",
    "\n",
    "def evaluate_model(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            (anchor_audio, anchor_text), anchor_label, (positive_audio, positive_text), (negative_audio, negative_text) = data\n",
    "\n",
    "            anchor_audio = anchor_audio.to(device)\n",
    "            anchor_text = anchor_text.to(device)\n",
    "            positive_audio = positive_audio.to(device)\n",
    "            positive_text = positive_text.to(device)\n",
    "            negative_audio = negative_audio.to(device)\n",
    "            negative_text = negative_text.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                anchor_text = bert_model(anchor_text.squeeze(1))[0]\n",
    "                positive_text = bert_model(positive_text.squeeze(1))[0]\n",
    "                negative_text = bert_model(negative_text.squeeze(1))[0]\n",
    "\n",
    "            anchor_out = model(anchor_audio, anchor_text)\n",
    "            positive_out = model(positive_audio, positive_text)\n",
    "            negative_out = model(negative_audio, negative_text)\n",
    "            loss = criterion(anchor_out, positive_out, negative_out)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss / len(data_loader)\n",
    "\n",
    "# Assuming the best model parameters are found and loaded into the model\n",
    "contrastive_model = MultimodalModel(audio_dim=audio_dim, text_dim=text_dim).to(device)\n",
    "contrastive_model.load_state_dict(torch.load('contrastive_model.pt'))\n",
    "\n",
    "# Create the loss function\n",
    "with open('best_pretrained_params.pickle', 'rb') as handle:\n",
    "    best_pretrained_params = pickle.load(handle)\n",
    "margin = best_pretrained_params['margin']\n",
    "criterion = TripletMarginLoss(margin=margin)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = evaluate_model(contrastive_model, criterion, data_loaders['test'])\n",
    "print('Test Loss: %.3f' % test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzaiJfhKyNVz"
   },
   "source": [
    "## Downstream Task Training\n",
    "\n",
    "Fine-tuning the contrastive pre-trained model to predict emotion and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nu4XHppLyQ30"
   },
   "outputs": [],
   "source": [
    "class DownstreamTaskDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio = self.dataset[idx]['audio'].unsqueeze(0)\n",
    "        text = self.dataset[idx]['text']['input_ids'].squeeze(-1)\n",
    "        sentiment = self.dataset[idx]['sentiment']\n",
    "        emotion = self.dataset[idx]['emotion']\n",
    "\n",
    "        return (audio, text), sentiment, emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uJTs5eC0c54"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = DownstreamTaskDataset(data_dicts['train'])\n",
    "dev_dataset = DownstreamTaskDataset(data_dicts['dev'])\n",
    "test_dataset = DownstreamTaskDataset(data_dicts['test'])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "data_loaders = {\n",
    "    'train': train_dataloader,\n",
    "    'dev': dev_dataloader,\n",
    "    'test': test_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnWSOFkF06fy"
   },
   "outputs": [],
   "source": [
    "class DownstreamModel(nn.Module):\n",
    "    def __init__(self, contrastive_model, num_sentiment_classes, num_emotion_classes, dropout_rate):\n",
    "        super(DownstreamModel, self).__init__()\n",
    "        self.contrastive_model = contrastive_model\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sentiment_fc = nn.Linear(256, num_sentiment_classes)\n",
    "        self.emotion_fc = nn.Linear(256, num_emotion_classes)\n",
    "\n",
    "    def forward(self, audio, text):\n",
    "        x = self.contrastive_model(audio, text)\n",
    "        x = self.dropout(x)\n",
    "        sentiment_out = self.sentiment_fc(x)\n",
    "        emotion_out = self.emotion_fc(x)\n",
    "        return sentiment_out, emotion_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIkxcJ-B2h4R"
   },
   "outputs": [],
   "source": [
    "#@title Training the Final Models\n",
    "\n",
    "num_epochs = 15\n",
    "print_every = 5\n",
    "\n",
    "def objective(trial):\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0, 0.5)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    sentiment_loss_weight = trial.suggest_uniform('sentiment_loss_weight', 0.1, 1.0)\n",
    "    emotion_loss_weight = trial.suggest_uniform('emotion_loss_weight', 0.1, 1.0)\n",
    "\n",
    "    multimodal_model = DownstreamModel(contrastive_model=contrastive_model, num_sentiment_classes=3, num_emotion_classes=7, dropout_rate).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(multimodal_model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        multimodal_model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, sentiment_label, emotion_label) in tqdm(enumerate(data_loaders['train'], 0)):\n",
    "            inputs = tuple(input.to(device) for input in inputs)\n",
    "            inputs[1] = bert_model(inputs[1].squeeze(1))[0] # Text input through BERT\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            sentiment_out, emotion_out = multimodal_model(*inputs)\n",
    "            sentiment_loss = criterion(sentiment_out, sentiment_label.to(device))\n",
    "            emotion_loss = criterion(emotion_out, emotion_label.to(device))\n",
    "            loss = sentiment_loss_weight * sentiment_loss + emotion_loss_weight * emotion_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': running_loss / (i+1)})\n",
    "        if epoch % print_every == 0:\n",
    "            print('[%d] loss: %.3f' % (epoch + 1, running_loss / print_every))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    multimodal_model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, sentiment_label, emotion_label) in enumerate(data_loaders['dev'], 0):\n",
    "            inputs = tuple(input.to(device) for input in inputs)\n",
    "\n",
    "            sentiment_out, emotion_out = multimodal_model(*inputs)\n",
    "            sentiment_loss = criterion(sentiment_out, sentiment_label.to(device))\n",
    "            emotion_loss = criterion(emotion_out, emotion_label.to(device))\n",
    "            loss = sentiment_loss_weight * sentiment_loss + emotion_loss_weight * emotion_loss\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    if trial.value == study.best_value:\n",
    "        torch.save(multimodal_model.state_dict(), 'best_model.pth')\n",
    "    return val_loss / len(data_loaders['dev'])\n",
    "\n",
    "# Load pre-trained model\n",
    "if (model is not None):\n",
    "  contrastive_model = model\n",
    "else:\n",
    "  contrastive_model = MultimodalModel(audio_dim=audio_dim, text_dim=text_dim).to(device)\n",
    "  contrastive_model.load_state_dict(torch.load('contrastive_model.pth'))\n",
    "\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(green, \"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"Value: {trial.value}\")\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Save the best parameters\n",
    "best_params = study.best_params\n",
    "with open('best_params.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_params, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3ZZV_Su3M9c"
   },
   "outputs": [],
   "source": [
    "#@title Evaluate Model\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Initialize lists to hold labels and predictions\n",
    "sentiment_labels = []\n",
    "sentiment_preds = []\n",
    "emotion_labels = []\n",
    "emotion_preds = []\n",
    "\n",
    "with open('best_params.pickle', 'rb') as handle:\n",
    "    best_params = pickle.load(handle)\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "\n",
    "# Load pre-trained model\n",
    "if contrastive_model is None:\n",
    "  contrastive_model = MultimodalModel(audio_dim=audio_dim, text_dim=text_dim).to(device)\n",
    "  contrastive_model.load_state_dict(torch.load('contrastive_model.pth'))\n",
    "\n",
    "multimodal_model = DownstreamModel(contrastive_model=contrastive_model, \n",
    "                                   num_sentiment_classes=3, \n",
    "                                   num_emotion_classes=7, \n",
    "                                   dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "multimodal_model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "multimodal_model.eval()\n",
    "\n",
    "total = 0\n",
    "sentiment_correct = 0\n",
    "emotion_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, sentiment_label, emotion_label) in enumerate(data_loaders['test'], 0):\n",
    "        inputs = tuple(input.to(device) for input in inputs)\n",
    "        \n",
    "        sentiment_out, emotion_out = multimodal_model(*inputs)\n",
    "        _, sentiment_predicted = torch.max(sentiment_out.data, 1)\n",
    "        _, emotion_predicted = torch.max(emotion_out.data, 1)\n",
    "        total += sentiment_label.size(0)\n",
    "        sentiment_correct += (sentiment_predicted == sentiment_label.to(device)).sum().item()\n",
    "        emotion_correct += (emotion_predicted == emotion_label.to(device)).sum().item()\n",
    "        sentiment_labels.extend(sentiment_label.cpu().numpy())\n",
    "        sentiment_preds.extend(sentiment_predicted.cpu().numpy())\n",
    "        emotion_labels.extend(emotion_label.cpu().numpy())\n",
    "        emotion_preds.extend(emotion_predicted.cpu().numpy())\n",
    "\n",
    "print(green, 'Accuracy of the sentiment prediction on the test data: %d %%' % (100 * sentiment_correct / total))\n",
    "print('Accuracy of the emotion prediction on the test data: %d %%' % (100 * emotion_correct / total))\n",
    "\n",
    "print('Classification report for sentiment prediction:')\n",
    "print(classification_report(sentiment_labels, sentiment_preds))\n",
    "\n",
    "print('Classification report for emotion prediction:')\n",
    "print(classification_report(emotion_labels, emotion_preds))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
